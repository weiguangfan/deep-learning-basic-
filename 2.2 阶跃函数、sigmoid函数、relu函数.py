"""
刚才登场的 h（x）函数会将输入信号的总和转换为输出信号，这种函数一般称为激活函数（activation function）。
如“激活”一词所示，激活函数的作用在于决定如何来激活输入信号的总和。
激活函数是连接感知机和神经网络的桥梁。

“朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数的模型。
“多层感知机”是指神经网络，即使用 sigmoid 函数（后述）等平滑的激活函数的多层网络。
阶跃函数是指以阈值为界,一旦输入超过阈值，就切换输出的函数。
因此，可以说感知机中使用了阶跃函数作为激活函数。
也就是说，在激活函数的众多候选函数中，感知机使用了阶跃函数。
阶跃函数
h(x) = 0 (x <= 0)
h(x) = 1 (x > 0)
当输入超过 0 时，输出 1，否则输出 0。
阶跃函数数学表达式的实现:
这个实现简单、易于理解，但是参数 x 只能接受实数（浮点数）。
也就是说，允许形如 step_function(3.0) 的调用，但不允许参数取 NumPy 数组，例如step_function(np.array([1.0, 2.0]))。
"""
import numpy as np

# def step_function(x):
#     if x > 0:  # 参数 x 只能接受实数（浮点数）。
#         return 1
#     else:
#         return 0


# print(step_function(3.0))
# print(step_function(np.array([1.0, 2.0])))  # 会报错

"""
改写为支持numpy数组
对 NumPy 数组进行不等号运算后，数组的各个元素都会进行不等号运算，生成一个布尔型数组。
这里，数组 x 中大于 0 的元素被转换为 True，小于等于 0的元素被转换为 False，从而生成一个新的数组 y。
数组 y 是一个布尔型数组，但是我们想要的阶跃函数是会输出 int 型的 0或 1 的函数。
因此，需要把数组 y 的元素类型从布尔型转换为 int 型。
可以用 astype() 方法转换 NumPy 数组的类型。
astype() 方法通过参数指定期望的类型，这个例子中是 np.int 型。
Python 中将布尔型转换为int 型后，True 会转换为 1，False 会转换为 0。
"""


# def step_function(x):
#     y = x > 0  # 允许参数取 NumPy 数组
#     return y.astype(np.int)


# x = np.array([-1.0, 1.0, 2.0])
# print(x)
# y = x > 0
# print(y)
# print(step_function(y))
# y = y.astype(np.int)
# print(y)


"""
阶跃函数的图形
阶跃函数以 0 为界，输出从 0 切换为 1（或者从 1 切换为0）。
它的值呈阶梯式变化，所以称为阶跃函数。
阶跃函数以 0 为界，输出发生急剧性的变化。
阶跃函数只能返回 0 或 1
感知机中神经元之间流动的是 0 或 1 的二元信号
输出信号的值都在 0 到 1 之间。

用图来表示上面定义的阶跃函数:
np.arange(-5.0, 5.0, 0.1) 在 -5.0 到 5.0 的范围内，以 0.1 为单位，生成 NumPy 数组（[-5.0, -4.9,..., 4.9]）。
step_function() 以该NumPy 数组为参数，对数组的各个元素执行阶跃函数运算，并以数组形式返回运算结果。
对数组 x、y 进行绘图
"""
import matplotlib.pyplot as plt


# def step_function(x):
#     return np.array(x > 0, dtype=np.int)


# x = np.arange(-5.0, 5.0, 0.1)
# y = step_function(x)
# plt.plot(x, y)
# plt.ylim(-0.1, 1.1)  # 指定y轴的范围
# plt.show()

"""
sigmoid 函数（sigmoid function）:
h(x) = 1/(1 + math.exp(-x))
exp(-x) 表示 e**(-x) 的意思。e 是纳皮尔常数 2.7182 ...。
sigmoid 函数看上去有些复杂，但它也仅仅是个函数而已。
而函数就是给定某个输入后，会返回某个输出的转换器。
比如，向 sigmoid 函数输入1.0 或 2.0 后，就会有某个值被输出，类似 h(1.0) = 0.731 ...、h(2.0) =0.880 ... 这样。
神经网络中用 sigmoid 函数作为激活函数，进行信号的转换，转换后的信号被传送给下一个神经元。
实际上，上一章介绍的感知机和接下来要介绍的神经网络的主要区别就在于这个激活函数。
其他方面，比如神经元的多层连接的构造、信号的传递方法等，基本上和感知机是一样的。

sigmoid 函数的实现:
np.exp(-x) 对应 exp(-x)。
这个实现没有什么特别难的地方，但是要注意参数 x 为 NumPy 数组时，结果也能被正确计算。
参数 x 为 NumPy 数组时，这个sigmoid 函数中输出一个 NumPy 数组
之所以 sigmoid 函数的实现能支持 NumPy 数组，秘密就在于 NumPy 的广播功能（1.5.5 节）。
根据 NumPy 的广播功能，如果在标量和 NumPy 数组之间进行运算，则标量会和 NumPy 数组的各个元素进行运算。

在这个例子中，标量（例子中是 1.0）和 NumPy 数组之间进行了数值运算（+、/ 等）。
结果，标量和 NumPy 数组的各个元素进行了运算，运算结果以NumPy 数组的形式被输出。
刚才的 sigmoid 函数的实现也是如此，因为np.exp(-x) 会生成 NumPy 数组，所以 1 / (1 + np.exp(-x)) 的运算将会在NumPy 数组的各个元素间进行。
"""


# def sigmoid(x):
#     return 1/(1 + np.exp(-x))


# x = np.array([-1.0, 1.0, 2.0])
# print(sigmoid(x))

# t = np.array([1.0, 2.0, 3.0])
# print((1.0 + t))
# print((1.0 / t))

"""
sigmoid 函数画在图上
画图的代码和刚才的阶跃函数的代码几乎是一样的，唯一不同的地方是把输出 y 的函数换成了 sigmoid 函数。
sigmoid 函数是一条平滑的曲线，输出随着输入发生连续性的变化。
sigmoid 函数可以返回0.731 ...、0.880 ... 等实数
神经网络中流动的是连续的实数值信号。
输出信号的值都在 0 到 1 之间。
"""

# x = np.arange(-5.0, 5.0, 0.1)
# y = sigmoid(x)
# plt.plot(x, y)
# plt.ylim(-0.1, 1.1)  # 指定y轴的范围
# plt.show()

"""
sigmoid 函数和阶跃函数的比较
首先注意到的是“平滑性”的不同。
sigmoid 函数是一条平滑的曲线，输出随着输入发生连续性的变化。
而阶跃函数以 0 为界，输出发生急剧性的变化。
sigmoid 函数的平滑性对神经网络的学习具有重要意义。

另一个不同点是，相对于阶跃函数只能返回 0 或 1，sigmoid 函数可以返回0.731 ...、0.880 ... 等实数（这一点和刚才的平滑性有关）。
也就是说，感知机中神经元之间流动的是 0 或 1 的二元信号，而神经网络中流动的是连续的实数值信号。

接着说一下阶跃函数和 sigmoid 函数的共同性质。
阶跃函数和 sigmoid 函数虽然在平滑性上有差异，但是如果从宏观视角看图 3-8，可以发现它们具有相似的形状。
实际上，两者的结构均是“输入小时，输出接近 0（为 0）；随着输入增大，输出向 1 靠近（变成 1）”。
也就是说，当输入信号为重要信息时，阶跃函数和 sigmoid 函数都会输出较大的值；
当输入信号为不重要的信息时，两者都输出较小的值。
还有一个共同点是，不管输入信号有多小，或者有多大，输出信号的值都在 0 到 1 之间。

阶跃函数和 sigmoid 函数还有其他共同点，就是两者均为非线性函数。
sigmoid 函数是一条曲线，阶跃函数是一条像阶梯一样的折线，两者都属于非线性的函数。

在介绍激活函数时，经常会看到“非线性函数”和“线性函数”等术语。函数本来是输入某个值后会返回一个值的转换器。
向这个转换器输入某个值后，输出值是输入值的常数倍的函数称为线性函数（用数学式表示为 h(x)= cx。c 为常数）。
因此，线性函数是一条笔直的直线。
而非线性函数，顾名思义，指的是不像线性函数那样呈现出一条直线的函数。

神经网络的激活函数必须使用非线性函数。
换句话说，激活函数不能使用线性函数。
为什么不能使用线性函数呢？
因为使用线性函数的话，加深神经网络的层数就没有意义了。

线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。
为了具体地（稍微直观地）理解这一点，我们来思考下面这个简单的例子。
这里我们考虑把线性函数 h(x) = cx 作为激活函数，把 y(x) =h(h(h(x))) 的运算对应 3 层神经网络 。
这个运算会进行 y(x) = c × c × c× x 的乘法运算，但是同样的处理可以由 y(x) = ax（注意，a = c**3）这一次乘法运算（即没有隐藏层的神经网络）来表示。
如本例所示，使用线性函数时，无法发挥多层网络带来的优势。
因此，为了发挥叠加层所带来的优势，激活函数必须使用非线性函数。

"""


"""
在神经网络发展的历史上，sigmoid 函数很早就开始被使用了，而最近则主要使用ReLU（Rectified Linear Unit）函数。
ReLU 函数在输入大于 0 时，直接输出该值；在输入小于等于 0 时，输出0.
h(x) = x (x>0)
h(x) = 0 (x<=0)
ReLU 函数是一个非常简单的函数。

relu函数的实现
这里使用了 NumPy 的 maximum 函数。
maximum 函数会从输入的数值中选择较大的那个值进行输出。

这里使用了 NumPy 的 maximum 函数。maximum 函数会从输入的数值中选择较大的那个值进行输出。
"""


# def relu(x):
#     return np.maximum(0, x)


# a = np.random.randn(2, 3)
# print(a)
# b = np.maximum(0, a)
# print(b)
