"""
CNN 中出现了一些特有的术语，比如填充、步幅等。
此外，各层中传递的数据是有形状的数据（比如，3 维数据），这与之前的全连接网络不同，因此刚开始学习 CNN 时可能会感到难以理解。
本节我们将花点时间，认真学习一下 CNN 中使用的卷积层的结构。

之前介绍的全连接的神经网络中使用了全连接层（Affine 层）。
在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定。
全连接层存在什么问题呢？
那就是数据的形状被“忽视”了。
比如，输入数据是图像时，图像通常是高、长、通道方向上的 3 维形状。
但是，向全连接层输入时，需要将 3 维数据拉平为 1 维数据。
实际上，
前面提到的使用了 MNIST 数据集的例子中，输入图像就是 1 通道、高 28 像素、长 28 像素的（1, 28, 28）形状，
但却被排成 1 列，以 784 个数据的形式输入到最开始的 Affine 层。


图像是 3 维形状，这个形状中应该含有重要的空间信息。
比如，空间上邻近的像素为相似的值、
RBG 的各个通道之间分别有密切的关联性、
相距较远的像素之间没有什么关联等，
3 维形状中可能隐藏有值得提取的本质模式。
但是，因为全连接层会忽视形状，将全部的输入数据作为相同的神经元（同一维度的神经元）处理，所以无法利用与形状相关的信息。

而卷积层可以保持形状不变。
当输入数据是图像时，卷积层会以 3 维数据的形式接收输入数据，并同样以 3 维数据的形式输出至下一层。
因此，在 CNN 中，可以（有可能）正确理解图像等具有形状的数据。

另外，CNN 中，有时将卷积层的输入输出数据称为特征图（feature map）。
其中，卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。
本书中将“输入输出数据”和“特征图”作为含义相同的词使用。

卷积层进行的处理就是卷积运算。
卷积运算相当于图像处理中的“滤波器运算”。
在介绍卷积运算时，我们来看一个具体的例子（图 7-3）。

图 7-3　卷积运算的例子：用“*”符号表示卷积运算

如图 7-3 所示，卷积运算对输入数据应用滤波器。
在这个例子中，输入数据是有高长方向的形状的数据，滤波器也一样，有高长方向上的维度。
假设用（height, width）表示数据和滤波器的形状，则在本例中，输入大小是 (4,4)，滤波器大小是 (3, 3)，输出大小是 (2, 2)。
另外，有的文献中也会用“核”这个词来表示这里所说的“滤波器”。

现在来解释一下图 7-3 的卷积运算的例子中都进行了什么样的计算。
图 7-4中展示了卷积运算的计算顺序。

图 7-4　卷积运算的计算顺序

对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用。
这里所说的窗口是指图 7-4 中灰色的 3 × 3 的部分。
如图 7-4 所示，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积累加运算）。
然后，将这个结果保存到输出的对应位置。
将这个过程在所有位置都进行一遍，就可以得到卷积运算的输出。

在全连接的神经网络中，除了权重参数，还存在偏置。
CNN 中，滤波器的参数就对应之前的权重。
并且，CNN 中也存在偏置。
图 7-3 的卷积运算的例子一直展示到了应用滤波器的阶段。
包含偏置的卷积运算的处理流如图 7-5 所示。
如图 7-5 所示，向应用了滤波器的数据加上了偏置。
偏置通常只有 1 个（1× 1）（本例中，相对于应用了滤波器的 4 个数据，偏置只有 1 个），这个值会被加到应用了滤波器的所有元素上。


图 7-5　卷积运算的偏置：向应用了滤波器的元素加上某个固定值（偏置）

在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（比如 0等），这称为填充（padding），是卷积运算中经常会用到的处理。
比如，在图 7-6 的例子中，对大小为 (4, 4) 的输入数据应用了幅度为 1 的填充。
“幅度为 1的填充”是指用幅度为 1 像素的 0 填充周围。

图 7-6　卷积运算的填充处理：向输入数据的周围填入 0（图中用虚线表示填充，并省略了填充的内容“0”）

如图 7-6 所示，通过填充，大小为 (4, 4) 的输入数据变成了 (6, 6) 的形状。
然后，应用大小为 (3, 3) 的滤波器，生成了大小为 (4, 4) 的输出数据。
这个例子中将填充设成了 1，不过填充的值也可以设置成 2、3 等任意的整数。
在图7-5 的例子中，如果将填充设为 2，则输入数据的大小变为 (8, 8)；如果将填充设为 3，则大小变为 (10, 10)。


使用填充主要是为了调整输出的大小。
比如，对大小为 (4, 4) 的输入数据应用 (3, 3) 的滤波器时，输出大小变为 (2, 2)，相当于输出大小比输入大小缩小了 2 个元素。
这在反复进行多次卷积运算的深度网络中会成为问题。
为什么呢？
因为如果每次进行卷积运算都会缩小空间，那么在某个时刻输出大小就有可能变为 1，导致无法再应用卷积运算。
为了避免出现这样的情况，就要使用填充。
在刚才的例子中，将填充的幅度设为 1，那么相对于输入大小 (4, 4)，输出大小也保持为原来的 (4, 4)。
因此，卷积运算就可以在保持空间大小不变的情况下将数据传给下一层。

应用滤波器的位置间隔称为步幅（stride）。
之前的例子中步幅都是 1，如果将步幅设为 2，则如图 7-7 所示，应用滤波器的窗口的间隔变为 2 个元素。


图 7-7　步幅为 2 的卷积运算的例子

在图 7-7 的例子中，对输入大小为 (7, 7) 的数据，以步幅 2 应用了滤波器。
通过将步幅设为 2，输出大小变为 (3, 3)。
像这样，步幅可以指定应用滤波器的间隔。


综上，增大步幅后，输出大小会变小。
而增大填充后，输出大小会变大。
如果将这样的关系写成算式，会如何呢？
接下来，我们看一下对于填充和步幅，如何计算输出大小。


这里，假设输入大小为 (H, W)，
滤波器大小为 (FH, FW)，
输出大小为 (OH,OW)，
填充为 P，
步幅为 S。
此时，输出大小可通过式 (7.1) 进行计算。


OH = (H + 2P - FH)/S + 1
OW = (W + 2P - FW)/S + 1

现在，我们使用这个算式，试着做几个计算。
例 1：
图 7-6 的例子输入大小：(4, 4)；填充：1；步幅：1；滤波器大小：(3, 3)

OH = 4
OW = 4

例 2：图 7-7 的例子输入大小：(7, 7)；填充：0；步幅：2；滤波器大小：(3, 3)

OH = 3
OW = 3


例 3输入大小：(28, 31)；填充：2；步幅：3；滤波器大小：(5, 5)

OH = 10
OW = 11

如这些例子所示，
通过在式（7.1）中代入值，就可以计算输出大小。
这里需要注意的是，
虽然只要代入值就可以计算输出大小，但是所设定的值必须使式（7.1）中的(H + 2P - FH)/S  和(W + 2P - FW)/S  分别可以除尽。
当输出大小无法除尽时（结果是小数时），需要采取报错等对策。
顺便说一下，根据深度学习的框架的不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行。


之前的卷积运算的例子都是以有高、长方向的 2 维形状为对象的。
但是，图像是 3 维数据，除了高、长方向之外，还需要处理通道方向。
这里，我们按照与之前相同的顺序，看一下对加上了通道方向的 3 维数据进行卷积运算的例子。

图 7-8 是卷积运算的例子，图 7-9 是计算顺序。
这里以 3 通道的数据为例，展示了卷积运算的结果。
和 2 维数据时（图 7-3 的例子）相比，可以发现纵深方向（通道方向）上特征图增加了。
通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。

图 7-8　对 3 维数据进行卷积运算的例子


图 7-9　对 3 维数据进行卷积运算的计算顺序

需要注意的是，在 3 维数据的卷积运算中，输入数据和滤波器的通道数要设为相同的值。
在这个例子中，输入数据和滤波器的通道数一致，均为 3。
滤波器大小可以设定为任意值（不过，每个通道的滤波器大小要全部相同）。
这个例子中滤波器大小为 (3, 3)，但也可以设定为 (2, 2)、(1, 1)、(5, 5) 等任意值。
再强调一下，通道数只能设定为和输入数据的通道数相同的值（本例中为 3）。


将数据和滤波器结合长方体的方块来考虑，3 维数据的卷积运算会很容易理解。
方块是如图 7-10 所示的 3 维长方体。
把 3 维数据表示为多维数组时，书写顺序为（channel, height, width）。
比如，通道数为 C、高度为 H、长度为 W的数据的形状可以写成（C, H, W）。
滤波器也一样，要按（channel, height,width）的顺序书写。
比如，通道数为 C、滤波器高度为 FH（Filter Height）、长度为 FW（Filter Width）时，可以写成（C, FH, FW）。

图 7-10　结合方块思考卷积运算。请注意方块的形状


在这个例子中，数据输出是 1 张特征图。
所谓 1 张特征图，换句话说，就是通道数为 1 的特征图。
那么，如果要在通道方向上也拥有多个卷积运算的输出，该怎么做呢？
为此，就需要用到多个滤波器（权重）。
用图表示的话，如图 7-11 所示。

图 7-11　基于多个滤波器的卷积运算的例子

图 7-11 中，通过应用 FN 个滤波器，输出特征图也生成了 FN 个。
如果将这FN 个特征图汇集在一起，就得到了形状为 (FN, OH, OW) 的方块。
将这个方块传给下一层，就是 CNN 的处理流。

如图 7-11 所示，关于卷积运算的滤波器，也必须考虑滤波器的数量。
因此，作为 4 维数据，滤波器的权重数据要按 (output_channel, input_channel,height, width) 的顺序书写。
比如，通道数为 3、大小为 5 × 5 的滤波器有20 个时，可以写成 (20, 3, 5, 5)。


图 7-12　卷积运算的处理流（追加了偏置项）

卷积运算中（和全连接层一样）存在偏置。
在图 7-11 的例子中，如果进一步追加偏置的加法运算处理，则结果如下面的图 7-12 所示。
图 7-12 中，每个通道只有一个偏置。
这里，偏置的形状是 (FN, 1, 1)，滤波器的输出结果的形状是 (FN, OH, OW)。
这两个方块相加时，要对滤波器的输出结果 (FN, OH, OW) 按通道加上相同的偏置值。
另外，不同形状的方块相加时，可以基于 NumPy 的广播功能轻松实现（1.5.5 节）。

神经网络的处理中进行了将输入数据打包的批处理。
之前的全连接神经网络的实现也对应了批处理，通过批处理，能够实现处理的高效化和学习时对 mini-batch 的对应。


图 7-13　卷积运算的处理流（批处理）

我们希望卷积运算也同样对应批处理。
为此，需要将在各层间传递的数据保存为 4 维数据。
具体地讲，就是按 (batch_num, channel, height, width) 的顺序保存数据。
比如，将图 7-12 中的处理改成对 N 个数据进行批处理时，数据的形状如图 7-13 所示。
图 7-13 的批处理版的数据流中，在各个数据的开头添加了批用的维度。
像这样，数据作为 4 维的形状在各层间传递。
这里需要注意的是，网络间传递的是 4维数据，对这 N 个数据进行了卷积运算。
也就是说，批处理将 N 次的处理汇总成了 1 次进行。












"""