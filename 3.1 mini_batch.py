"""
机器学习使用训练数据进行学习。
使用训练数据进行学习，严格来说，就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数。
因此，计算损失函数时必须将所有的训练数据作为对象。
也就是说，如果训练数据有 100 个的话，我们就要把这 100 个损失函数的总和作为学习的指标。

前面介绍的损失函数的例子中考虑的都是针对单个数据的损失函数。
如果要求所有训练数据的损失函数的总和，以交叉熵误差为例，可以写成下面的式（4.3）。

E = - 1/N sum(sum(tnk * math.log(ynk)))

这里，假设数据有 N 个，tnk 表示第 n 个数据的第 k 个元素的值（ ynk是神经网络的输出，tnk 是监督数据）。
式子虽然看起来有一些复杂，其实只是把求单个数据的损失函数的式（4.2）扩大到了 N 份数据，不过最后还要除以 N 进行正规化。
通过除以 N，可以求单个数据的“平均损失函数”。
通过这样的平均化，可以获得和训练数据的数量无关的统一指标。
比如，即便训练数据有 1000 个或 10000个，也可以求得单个数据的平均损失函数。

另外，MNIST 数据集的训练数据有 60000 个，如果以全部数据为对象求损失函数的和，则计算过程需要花费较长的时间。
再者，如果遇到大数据，数据量会有几百万、几千万之多，这种情况下以全部数据为对象计算损失函数是不现实的。
因此，我们从全部数据中选出一部分，作为全部数据的“近似”。
神经网络的学习也是从训练数据中选出一批数据（称为 mini-batch, 小批量），
然后对每个 mini-batch 进行学习。
比如，从 60000 个训练数据中随机选择 100 笔，再用这 100笔数据进行学习。
这种学习方式称为 mini-batch 学习。

下面我们来编写从训练数据中随机选择指定个数的数据的代码，以进行 mini-batch 学习。
在这之前，先来看一下用于读入 MNIST 数据集的代码。

mini-batch学习：利用一部分数据近似整体
"""

import os
import sys

sys.path.append(os.pardir)
from dataset.mnist import load_mnist
import numpy as np

# 加载数据
(x_train, t_train), (x_test, t_test), = load_mnist(one_hot_label=True, normalize=True)
print(x_train.shape)  # (60000, 784) 输入数据784维
print(t_train.shape)  # (60000, 10) 监督数据10维
"""
第 3 章介绍过，load_mnist 函数是用于读入 MNIST 数据集的函数。
这个函数在本书提供的脚本 dataset/mnist.py 中，它会读入训练数据和测试数据。
读入数据时，通过设定参数 one_hot_label=True，可以得到 one-hot 表示（即仅正确解标签为 1，其余为 0 的数据结构）。

读入上面的 MNIST 数据后，训练数据有 60000 个，输入数据是 784 维（28× 28）的图像数据，监督数据是 10 维的数据。
因此，上面的x_train、t_train 的形状分别是 (60000, 784) 和 (60000, 10)。
那么，如何从这个训练数据中随机抽取 10 笔数据呢？
我们可以使用 NumPy的 np.random.choice()，写成如下形式。

"""
# 从训练数据随机抽取10笔数据
train_size = x_train.shape[0]
print(train_size)
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)
print(batch_mask)
x_batch = x_train[batch_mask]  # [56238 14055 32411 41573 48975 37604 50257 45892 41825 49994] 索引数组
print(x_batch.shape)  # (10, 784)
t_batch = t_train[batch_mask]
print(t_batch.shape)  # (10, 10)
print(t_batch.size)  # 元素的总数
"""
使用 np.random.choice() 可以从指定的数字中随机选择想要的数字。
比如，np.random.choice(60000, 10) 会从 0 到 59999 之间随机选择 10 个数字。
如下面的实际代码所示，我们可以得到一个包含被选数据的索引的数组。

"""
# print(np.random.choice(60000, 10))

"""
之后，我们只需指定这些随机选出的索引，取出 mini-batch，然后使用这个mini-batch 计算损失函数即可。

计算电视收视率时，并不会统计所有家庭的电视机，而是仅以那些被选中的家庭为统计对象。
比如，通过从关东地区随机选择 1000 个家庭计算收视率，可以近似地求得关东地区整体的收视率。
这 1000 个家庭的收视率，虽然严格上不等于整体的收视率，但可以作为整体的一个近似值。
和收视率一样，mini-batch 的损失函数也是利用一部分样本数据来近似地计算整体。
也就是说，用随机选择的小批量数据（mini-batch）作为全体训练数据的近似值。

如何实现对应 mini-batch 的交叉熵误差呢？
只要改良一下之前实现的对应单个数据的交叉熵误差就可以了。
这里，我们来实现一个可以同时处理单个数据和批量数据（数据作为 batch 集中输入）两种情况的函数。

"""
# def cross_entropy_error(y, t):
#     delta = 1e-7
#     if y.ndim == 1:
#         t = t.reshape(1, t.size)
#         y = y.reshape(1, y.size)
#     # 抽出正确标签对应的输出值
#     batch_size = y.shape[0]
#     print('batch_size: ', batch_size)
#     return -np.sum(t * np.log(y + delta))/batch_size
"""
这里，y 是神经网络的输出，t 是监督数据。
y 的维度为 1 时，即求单个数据的交叉熵误差时，需要改变数据的形状。
并且，当输入为 mini-batch 时，要用batch 的个数进行正规化，计算单个数据的平均交叉熵误差。

此外，当监督数据是标签形式（非 one-hot 表示，而是像“2”“7”这样的标签）时，交叉熵误差可通过如下代码实现。
实现的要点是，由于 one-hot 表示中 t 为 0 的元素的交叉熵误差也为 0，因此针对这些元素的计算可以忽略。
换言之，如果可以获得神经网络在正确解标签处的输出，就可以计算交叉熵误差。
因此，
t 为 one-hot 表示时通过 t * np.log(y) 计算的地方，
在 t 为标签形式时，可用 np.log( y[np.arange(batch_size), t] ) 实现相同的处理（为了便于观察，这里省略了微小值 1e-7）。

作为参考，简单介绍一下np.log( y[np.arange(batch_size), t])。
np.arange (batch_size) 会生成一个从 0 到 batch_size-1 的数组。
比如当 batch_size 为 5 时，np.arange(batch_size) 会生成一个 NumPy 数组[0, 1, 2, 3, 4]。
因为 t 中标签是以 [2, 7, 0, 9, 4] 的形式存储的，
所以y[np.arange(batch_size), t] 能抽出各个数据的正确解标签对应的神经网络的输出
（在这个例子中，y[np.arange(batch_size), t] 会生成 NumPy 数组[y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]]）。


"""


def cross_entropy_error(y, t):
    """mini-batch 交叉熵损失：单个数据的平均交叉熵损失"""
    delta = 1e-7
    # 单个数据的交叉熵损失，改变形状
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    batch_size = y.shape[0]
    # 单个数据的平均交叉熵损失
    return -np.sum(np.log(y[np.arange(batch_size), t] + delta)) / batch_size


"""
为何要设定损失函数上面我们讨论了损失函数，可能有人要问：
“为什么要导入损失函数呢？”
以数字识别任务为例，我们想获得的是能提高识别精度的参数，
特意再导入一个损失函数不是有些重复劳动吗？
也就是说，既然我们的目标是获得使识别精度尽可能高的神经网络，那不是应该把识别精度作为指标吗？
对于这一疑问，我们可以根据“导数”在神经网络学习中的作用来回答。
下一节中会详细说到，在神经网络的学习中，寻找最优参数（权重和偏置）时，要寻找使损失函数的值尽可能小的参数。
为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值。

假设有一个神经网络，现在我们来关注这个神经网络中的某一个权重参数。
此时，对该权重参数的损失函数求导，表示的是“如果稍微改变这个权重参数的值，损失函数的值会如何变化”。
如果导数的值为负，通过使该权重参数向正方向改变，可以减小损失函数的值；
反过来，如果导数的值为正，则通过使该权重参数向负方向改变，可以减小损失函数的值。
不过，当导数的值为 0 时，无论权重参数向哪个方向变化，损失函数的值都不会改变，此时该权重参数的更新会停在此处。
之所以不能用识别精度作为指标，是因为这样一来绝大多数地方的导数都会变为 0，导致参数无法更新。

话说得有点多了，我们来总结一下上面的内容。
在进行神经网络的学习时，不能将识别精度作为指标。
因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为 0。

为什么用识别精度作为指标时，参数的导数在绝大多数地方都会变成 0 呢？
为了回答这个问题，我们来思考另一个具体例子。
假设某个神经网络正确识别出了100 笔训练数据中的 32 笔，此时识别精度为 32 %。
如果以识别精度为指标，即使稍微改变权重参数的值，识别精度也仍将保持在 32 %，不会出现变化。
也就是说，仅仅微调参数，是无法改善识别精度的。
即便识别精度有所改善，它的值也不会像 32.0123 ... % 这样连续变化，而是变为 33 %、34 % 这样的不连续的、离散的值。
而如果把损失函数作为指标，则当前损失函数的值可以表示为 0.92543... 这样的值。
并且，如果稍微改变一下参数的值，对应的损失函数也会像0.93432 ... 这样发生连续性的变化。
识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值也是不连续地、突然地变化。
作为激活函数的阶跃函数也有同样的情况。
出于相同的原因，如果使用阶跃函数作为激活函数，神经网络的学习将无法进行。
如图 4-4 所示，阶跃函数的导数在绝大多数地方（除了 0 以外的地方）均为 0。
也就是说，如果使用了阶跃函数，那么即便将损失函数作为指标，参数的微小变化也会被阶跃函数抹杀，导致损失函数的值不会产生任何变化。

图 4-4　阶跃函数和 sigmoid 函数：阶跃函数的斜率在绝大多数地方都为0，而 sigmoid 函数的斜率（切线）不会为 0

阶跃函数就像“竹筒敲石”一样，只在某个瞬间产生变化。
而 sigmoid 函数，如图 4-4 所示，不仅函数的输出（竖轴的值）是连续变化的，曲线的斜率（导数）也是连续变化的。
也就是说，sigmoid 函数的导数在任何地方都不为 0。
这对神经网络的学习非常重要。
得益于这个斜率不会为 0 的性质，神经网络的学习得以正确进行。


"""
