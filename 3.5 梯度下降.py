"""
机器学习的主要任务是在学习时寻找最优参数。
同样地，神经网络也必须在学习时找到最优参数（权重和偏置）。
这里所说的最优参数是指损失函数取最小值时的参数。
但是，一般而言，损失函数很复杂，参数空间庞大，我们不知道它在何处能取得最小值。
而通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法。

这里需要注意的是，梯度表示的是各点处的函数值减小最多的方向。
因此，无法保证梯度所指的方向就是函数的最小值或者真正应该前进的方向。
实际上，在复杂的函数中，梯度指示的方向基本上都不是函数值最小处。

函数的极小值、最小值以及被称为鞍点（saddle point）的地方，梯度为 0。
极小值是局部最小值，也就是限定在某个范围内的最小值。
鞍点是从某个方向上看是极大值，从另一个方向上看则是极小值的点。
虽然梯度法是要寻找梯度为 0 的地方，但是那个地方不一定就是最小值（也有可能是极小值或者鞍点）。
此外，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入被称为“学习高原”的无法前进的停滞期。

虽然梯度的方向并不一定指向最小值，但沿着它的方向能够最大限度地减小函数的值。
因此，在寻找函数的最小值（或者尽可能小的值）的位置的任务中，要以梯度的信息为线索，决定前进的方向。

此时梯度法就派上用场了。
在梯度法中，函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进，如此反复，不断地沿梯度方向前进。
像这样，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法（gradient method）。
梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用。

根据目的是寻找最小值还是最大值，梯度法的叫法有所不同。
严格地讲，寻找最小值的梯度法称为梯度下降法（gradient descent method），寻找最大值的梯度法称为梯度上升法（gradient ascent method）。
但是通过反转损失函数的符号，求最小值的问题和求最大值的问题会变成相同的问题，因此“下降”还是“上升”的差异本质上并不重要。
一般来说，神经网络（深度学习）中，梯度法主要是指梯度下降法。

现在，我们尝试用数学式来表示梯度法，如式（4.7）所示。

x = x0 - η * e_f/e_x0
x = x1 - η * e_f/e_x1

式（4.7）的 η 表示更新量，在神经网络的学习中，称为学习率（learningrate）。
学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数。

式（4.7）是表示更新一次的式子，这个步骤会反复执行。
也就是说，每一步都按式（4.7）更新变量的值，通过反复执行此步骤，逐渐减小函数值。
虽然这里只展示了有两个变量时的更新过程，但是即便增加变量的数量，也可以通过类似的式子（各个变量的偏导数）进行更新。

学习率需要事先确定为某个值，比如 0.01 或 0.001。
一般而言，这个值过大或过小，都无法抵达一个“好的位置”。
在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了。


"""
import numpy as np

"""
问题：请用梯度法求 f(x0 + x1) = x[0]**2 + x[1]**2 的最小值。
"""


def function_2(x):
    """原函数"""
    print("x: ", x)
    return x[0] ** 2 + x[1] ** 2


def numerical_gradient(f, x):
    """遍历数组的每个元素，进行数值微分"""
    h = 1e-4
    grad = np.zeros_like(x)
    print("grad: ", grad)
    for idx in range(x.size):
        print("idx: ", idx)
        tmp_val = x[idx]
        print('i:tmp_val: ', idx, tmp_val)

        x[idx] = tmp_val + h
        fxh1 = f(x)
        print("idx:fxh1: ", idx, fxh1)

        x[idx] = tmp_val - h
        fxh2 = f(x)

        grad[idx] = (fxh1 - fxh2) / (2 * h)
        print("grad: ", grad)

        x[idx] = tmp_val
    return grad


"""
下面，我们用 Python 来实现梯度下降法。
如下所示，这个实现很简单。

参数 f 是要进行最优化的函数，init_x 是初始值，lr 是学习率 learningrate，step_num 是梯度法的重复次数。
numerical_gradient(f,x) 会求函数的梯度，用该梯度乘以学习率得到的值进行更新操作，由 step_num 指定重复的次数。

使用这个函数可以求函数的极小值，顺利的话，还可以求函数的最小值。
下面，我们就来尝试解决下面这个问题。

"""


def gradient_descent(f, init_x, lr=0.01, step_num=100):
    """梯度下降法："""
    x = init_x

    # 反复执行更新的式子
    for i in range(step_num):  # step_num梯度法的重复次数
        print("###" * 10)
        print("i: ", i)
        # 返回偏导向量组成的数组
        grad = numerical_gradient(f, x)

        # 更新一次的式子，lr学习率过大过小，都无法抵达一个好的位置
        x -= lr * grad
    return x


# 初始值
init_x = np.array([-3.0, 4.0])

# 使用梯度法寻找最小值，没怎么更新就结束了
# print(gradient_descent(function_2, init_x, lr=0.1, step_num=100))
"""
这里，设初始值为 (-3.0, 4.0)，开始使用梯度法寻找最小值。
最终的结果是(-6.1e-10, 8.1e-10)，非常接近 (0, 0)。
实际上，真的最小值就是 (0, 0)，所以说通过梯度法我们基本得到了正确结果。

如果用图来表示梯度法的更新过程，则如图 4-10 所示。
可以发现，原点处是最低的地方，函数的取值一点点在向其靠近。
这个图的源代码在 ch04/gradient_method.py 中（但ch04/gradient_method.py 不显示表示等高线的虚线）。

图 4-10　f(x0 + x1) = x[0]**2 + x[1]**2 的梯度法的更新过程：虚线是函数的等高线

前面说过，学习率过大或者过小都无法得到好的结果。
我们来做个实验验证一下。

实验结果表明，学习率过大的话，会发散成一个很大的值；
反过来，学习率过小的话，基本上没怎么更新就结束了。
也就是说，设定合适的学习率是一个很重要的问题。

像学习率这样的参数称为超参数。
这是一种和神经网络的参数（权重和偏置）性质不同的参数。
相对于神经网络的权重参数是通过训练数据和学习算法自动获得的，学习率这样的超参数则是人工设定的。
一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利进行的设定。



"""
# 学习率过大，lr=10.0，会发散成一个很大的值
# print(gradient_descent(function_2, init_x, lr=10, step_num=100))

# 学习率过小，lr=1e-10，
print(gradient_descent(function_2, init_x, lr=1e-10, step_num=100))
