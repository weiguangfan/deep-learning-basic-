"""
神经网络的正向传播中，为了计算加权信号的总和，使用了矩阵的乘积运算（NumPy 中是 np.dot()，具体请参照 3.3 节）。
比如，还记得我们用 Python进行了下面的实现吗？


"""
# 单个数据对象的affine层
# 计算图各个节点流动的是标量
import numpy as np
x = np.random.rand(2)  # 输入
w = np.random.rand(2,3)  # 权重
b = np.random.rand(3)  # 偏置

print(x.shape)
print(w.shape)
print(b.shape)

y = np.dot(x, w) + b
print(y)
print(y.shape)
"""
这里，X、W、B 分别是形状为 (2,)、(2, 3)、(3,) 的多维数组。
这样一来，神经元的加权和可以用 Y = np.dot(X, W) + B 计算出来。
然后，Y 经过激活函数转换后，传递给下一层。
这就是神经网络正向传播的流程。
此外，我们来复习一下，矩阵的乘积运算的要点是使对应维度的元素个数一致。
比如，如下面的图5-23 所示，X 和 W 的乘积必须使对应维度的元素个数一致。
另外，这里矩阵的形状用 (2, 3) 这样的括号表示（为了和 NumPy 的 shape 属性的输出一致）。

图 5-23　矩阵的乘积运算中对应维度的元素个数要保持一致

神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”。
因此，这里将进行仿射变换的处理实现为“Affine 层”。
几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算。

现在将这里进行的求矩阵的乘积与偏置的和的运算用计算图表示出来。
将乘积运算用“dot”节点表示的话，则 np.dot(X, W) + B 的运算可用图 5-24 所示的计算图表示出来。
另外，在各个变量的上方标记了它们的形状（比如，计算图上显示了 X 的形状为 (2,)，X.W 的形状为 (3,) 等）。

图 5-24　Affine 层的计算图（注意变量是矩阵，各个变量的上方标记了该变量的形状）

图 5-24 是比较简单的计算图，不过要注意X 、W、B 是矩阵（多维数组）。
之前我们见到的计算图中各个节点间流动的是标量，而这个例子中各个节点间传播的是矩阵。

现在我们来考虑图 5-24 的计算图的反向传播。
以矩阵为对象的反向传播，按矩阵的各个元素进行计算时，步骤和以标量为对象的计算图相同。
实际写一下的话，可以得到下式（这里省略了式（5.13）的推导过程）。

e_L/e_X = e_L/e_Y . W.T
e_L/e_W = X.T . e_L/e_Y

式（5.13）中 W.T 的 T 表示转置。
转置操作会把 W 的元素 (i, j) 换成元素 (j, i)。
用数学式表示的话，可以写成下面这样。

W = w11  w12  w13 
    w21  w22  w23
    
W.T = w11  w21
      w12  w22
      w13  w23

如式（5.14）所示，如果 W 的形状是 (2, 3)，W.T 的形状就是 (3, 2)。

现在，我们根据式（5.13），尝试写出计算图的反向传播，如图 5-25 所示。

图 5-25　Affine 层的反向传播：注意变量是多维数组。反向传播时各个变量的下方标记了该变量的形状

我们看一下图 2-25 的计算图中各个变量的形状。尤其要注意，X 和 e_L/e_X 形状相同，W 和 e_L/e_W 形状相同。
从下面的数学式可以很明确地看出 X 和 e_L/e_X 形状相同。

X = (x0,x1,x2,...,xn)
e_L/e_X = (e_L/e_X0,e_L/e_X1,...,e_L/e_Xn)

图 5-26　矩阵的乘积（“dot”节点）的反向传播可以通过组建使矩阵对应维度的元素个数一致的乘积运算而推导出来
为什么要注意矩阵的形状呢？因为矩阵的乘积运算要求对应维度的元素个数保持一致，通过确认一致性，就可以导出式（5.13）。
比如，e_L/e_Y 的形状是 (3,)，W 的形状是 (2, 3) 时，思考 e_L/e_Y 和 W.T 的乘积，使得 e_L/e_X 的形状为 (2,)（图 5-26）。
这样一来，就会自然而然地推导出式（5.13）。
"""


"""
前面介绍的 Affine层的输入 X 是以单个数据为对象的。
现在我们考虑 N 个数据一起进行正向传播的情况，也就是批版本的 Affine层。
先给出批版本的 Affine层的计算图，如图 5-27 所示。

图 5-27　批版本的 Affine 层的计算图

与刚刚不同的是，现在输入 X 的形状是 (N, 2)。
之后就和前面一样，在计算图上进行单纯的矩阵计算。
反向传播时，如果注意矩阵的形状，就可以和前面一样推导出 e_L/e_X 和 e_L/e_W 。

加上偏置时，需要特别注意。
正向传播时，偏置被加到 X.W 的各个数据上。
比如，N = 2（数据为 2 个）时，偏置会被分别加到这 2 个数据（各自的计算结果）上，具体的例子如下所示。

"""
# 批版本的affine层
# 计算图中各个节点流动是矩阵
# 求导变成了矩阵求导
x_dot_w = np.array([[0,0,0],[10,10,10]])
print(x_dot_w)
print(x_dot_w.shape)
b = np.array([1,2,3])
print(b.shape)
print(b)
# 正向传播时，偏置被加到x.w的各个数据上
y = x_dot_w + b
print(y)
print(y.shape)
"""
正向传播时，偏置会被加到每一个数据（第 1 个、第 2 个......）上。
因此，反向传播时，各个数据的反向传播的值需要汇总为偏置的元素。
用代码表示的话，如下所示。
"""

# 根据求导公式，
dy = np.array([[1,2,3],[4,5,6]])
print(dy.shape)
print(dy)
# 反向传播时，各个数据的反向传播的值需要汇总为偏置的元素
db = np.sum(dy, axis=0)
print(db.shape)
print(db)
"""
这个例子中，假定数据有 2 个（N = 2）。
偏置的反向传播会对这 2 个数据的导数按元素进行求和。
因此，这里使用了 np.sum() 对第 0 轴（以数据为单位的轴，axis=0）方向上的元素进行求和。

综上所述，Affine 的实现如下所示。
另外，common/layers.py 中的Affine 的实现考虑了输入数据为张量（四维数据）的情况，与这里介绍的稍有差别。

"""
class Affine(object):
    """矩阵乘积运算，改写为affine层，affine的实现"""
    def __init__(self, w, b):
        self.w = w
        self.b = b
        self.x = None
        self.dw = None
        self.db = None

    def forward(self, x):
        self.x = x
        out = np.dot(x, self.w) + self.b
        return out

    def backward(self, dout):
        dx = np.dot(dout, self.w.T)
        self.dw = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)
        return dx
